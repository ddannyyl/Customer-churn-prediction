{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#ML algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score,recall_score, RocCurveDisplay,precision_recall_curve, auc, classification_report\n",
    "\n",
    "# model optimization\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = r\"../raw/raw.csv\"\n",
    "df = pd.read_csv(csv)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]\n",
    "# Drop duplicates and reset index\n",
    "df = df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unused columns\n",
    "df = df.drop(\"customerID\", axis=1)\n",
    "\n",
    "# Convert to numeric\n",
    "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
    "df[\"TotalCharges\"] = df[\"TotalCharges\"].fillna(df[\"TotalCharges\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change columns into category type\n",
    "cat_cols = ['gender', 'Partner', 'Dependents',\n",
    "       'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "       'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
    "       'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df[cat_cols] = df[cat_cols].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.describe(include='category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  One-hot encoding\n",
    "df = pd.get_dummies(df, columns=cat_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check variable target distribution (churn percentage in dataset)\n",
    "pie_graph = df['Churn'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "for graph in pie_graph.texts:\n",
    "    graph.set_color(\"black\") \n",
    "plt.ylabel(\"\")\n",
    "plt.title(\"Churn Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=len(num_cols), figsize=(12, 4*len(num_cols)))\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.boxplot(data=df, x=col, ax=ax[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for outlier using z-score\n",
    "z_thresh = 3\n",
    "\n",
    "for col in num_cols:\n",
    "    mean = df[col].mean()\n",
    "    std = df[col].std()\n",
    "    z = (df[col] - mean) / std\n",
    "    outliers = (np.abs(z) > z_thresh).sum()\n",
    "    print(col, \"outliers:\", outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram to determine bins for Tenure\n",
    "df['tenure'].hist(bins=30, edgecolor='black')\n",
    "plt.xlabel('Tenure Period')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Tenure')\n",
    "plt.show()\n",
    "\n",
    "# Quick statistics\n",
    "df['tenure'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram to determine bins for MonthlyCharges\n",
    "df['MonthlyCharges'].hist(bins=30, edgecolor='black')\n",
    "plt.xlabel('MonthlyCharges')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of MonthlyCharges')\n",
    "plt.show()\n",
    "\n",
    "df['MonthlyCharges'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram to determine bins for TotalCharges\n",
    "df['TotalCharges'].hist(bins=30, edgecolor='black')\n",
    "plt.xlabel('TotalCharges')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of TotalCharges')\n",
    "plt.show()\n",
    "\n",
    "df['TotalCharges'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Churn_num\"] = df[\"Churn\"].astype(str).str.strip().map({\"Yes\": 1, \"No\": 0})\n",
    "df[\"Churn_num\"].isna().sum()\n",
    "\n",
    "feature_cols = df.select_dtypes(include=[\"number\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# Correlation heat map\n",
    "corr = df[feature_cols].corr()\n",
    "f, ax = plt.subplots(figsize=(18, 16))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "churn_corr = corr[[\"Churn_num\"]].drop(\"Churn_num\").sort_values(\"Churn_num\", ascending=False)\n",
    "sns.heatmap(churn_corr, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n",
    "            linewidths=0.2, linecolor=\"white\")\n",
    "plt.title(\"Correlation with Churn\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr = churn_corr[abs(churn_corr['Churn_num']) > 0.25]\n",
    "high_corr.sort_values(by='Churn_num', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Train-Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "X_raw = df_copy.drop(columns=['Churn_num', 'Churn'])\n",
    "\n",
    "X = X_raw\n",
    "y = df['Churn_num'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Train - x:', X_train.shape, 'y:', y_train.shape)\n",
    "print('Test - x:', X_test.shape, 'y:', y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling (Standardization Data) - scales feautres of data so that they have zero mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = df[num_cols]\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"column\": num_df.columns,\n",
    "    \"min\": num_df.min().values,\n",
    "    \"max\": num_df.max().values\n",
    "}).sort_values(by=\"min\")\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = {\n",
    "    'logistic_regression' : LogisticRegression(random_state = 42, max_iter = 10000),\n",
    "    'decision_tree' : DecisionTreeClassifier(max_depth = 5, random_state = 42),\n",
    "    'Random_forest' : RandomForestClassifier(n_estimators = 150, max_depth = 4, random_state = 42),\n",
    "    'GBDT' : GradientBoostingClassifier(n_estimators = 150, max_depth = 4, random_state = 42),\n",
    "    \"XGBoost\" : xgb.XGBClassifier(n_estimators = 200, max_depth = 5, random_state = 42)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train_scores = [] \n",
    "f1_test_scores = [] \n",
    "recall_test_scores = []\n",
    "\n",
    "model_names = list_of_models.keys()\n",
    "\n",
    "for model in model_names:\n",
    "    print(\"\\nFor Model:\", model)\n",
    "    list_of_models[model].fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nFor Training Set:\")\n",
    "    y_train_pred = list_of_models[model].predict(X_train)\n",
    "\n",
    "    f1_train = f1_score(y_train, y_train_pred, average='macro')\n",
    "    print(\"\\nMacro F1 Score:\", f1_train)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\") \n",
    "    cm = metrics.confusion_matrix(y_train, y_train_pred)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])\n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"For Test Set:\")\n",
    "\n",
    "    y_test_pred = list_of_models[model].predict(X_test)\n",
    "    \n",
    "    f1_test = f1_score(y_test, y_test_pred, average='macro')\n",
    "    print(\"\\nMacro F1 Score:\", f1_test)\n",
    "\n",
    "    recall_test_score = recall_score(y_test, y_test_pred, average='macro')\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])\n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "    \n",
    "    f1_train_scores.append(f1_train)\n",
    "    f1_test_scores.append(f1_test)\n",
    "    recall_test_scores.append(recall_test_score)\n",
    "\n",
    "results = []    \n",
    "\n",
    "for name, model in list_of_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred  = model.predict(X_test)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"F1 Train (macro)\": f1_score(y_train, y_train_pred, average=\"macro\"),\n",
    "        \"F1 Test (macro)\":  f1_score(y_test, y_test_pred, average=\"macro\"),\n",
    "        \"Recall Test (macro)\": recall_score(y_test, y_test_pred, average=\"macro\")\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"F1 Test (macro)\", ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: Logistic Regression has the best Test macro-F1 (0.761) and Recall is also highest\n",
    "Logistic Regression has the best Test macro-F1 (0.761) and Recall is also highest. Likely overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Logistic Regression properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=2000)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "y_test_prob = log_reg.predict_proba(X_test)[:, 1]  # probability of churn=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_report = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_report, display_labels=[0, 1])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "print(\"ROC-AUC:\", roc_auc)\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, y_test_prob)\n",
    "plt.title(\"ROC Curve (Test)\")\n",
    "plt.show()\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_test_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(\"PR-AUC:\", pr_auc)\n",
    "\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve (Test)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1, 10],\n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\"],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    LogisticRegression(max_iter=5000),\n",
    "    param_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV score:\", grid.best_score_)\n",
    "\n",
    "best_log_reg = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_log_reg.predict(X_test)\n",
    "y_test_prob = best_log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Test F1 Macro:\", f1_score(y_test, y_test_pred, average=\"macro\"))\n",
    "print(\"Test ROC-AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Churn\", \"Churn_num\"])\n",
    "y = df[\"Churn_num\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": best_log_reg.coef_[0]\n",
    "})\n",
    "\n",
    "coef_df[\"AbsCoeff\"] = coef_df[\"Coefficient\"].abs()\n",
    "coef_df = coef_df.sort_values(\"AbsCoeff\", ascending=False)\n",
    "\n",
    "coef_df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = LogisticRegression(**grid.best_params_, max_iter=5000)\n",
    "final_model.fit(X, y)  # train on full dataset now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, joblib\n",
    "\n",
    "joblib.dump(final_model, \"../../models/logistic_churn_model.pkl\")\n",
    "joblib.dump(X.columns.tolist(), \"../../models/model_features.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
